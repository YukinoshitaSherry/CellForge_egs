# Expert Panel Dialogue: Designing Perturbation Prediction Model

## Round 1

Data Scientist: When I look at sci-Plex drug6—188 compounds across four doses, ~650k cells from A549, K562, MCF7—the two biggest statistical features are extreme sparsity (>90% zeros) and subtle but real batch/hash artifacts. If we learn directly on raw counts, the model will chase noise. I propose we stabilize inputs with log2(CP10K+1), then z-score each gene using training cells only. We should drop genes present in fewer than 1% of cells and work with 3–5k highly variable genes. The rest can be reconstructed through a mapping if we need full outputs, but for drug6, keeping a manageable HVG set is essential for scaling.

Model Architect: Agreed. And I’d like to frame the prediction as a delta relative to baseline, not absolute expression. The model takes (x_baseline, compound ID, dose) and returns Δx; then we reconstruct x_pred = x_baseline + Δx. Delta learning aligns with how drugs alter cells and should generalize better across lines. Architecturally, I want a baseline encoder to obtain a latent z_control, a perturbation encoder to represent compound ID and dose, and then a geometry-aware transformation that maps z_control to a perturbed latent. I’m leaning toward an entropic optimal transport (OT) objective to guide that geometry, coupled with a conditional, invertible refiner (a small normalizing flow) to handle non-linearities. A modular decoder can then project the refined latent into Δx over the HVGs.

Deep Learning Engineer: If we do OT, we have to mind compute. Sinkhorn iterations explode with large batches. But we could restrict to moderate batch sizes and cap iterations around 40–60 with ε in, say, 0.05–0.2. As for the flow refiner, six masked affine coupling layers should be enough, each conditioned on the drug+dose embedding. Alternating binary masks will cover all dimensions. We’ll also regularize the Jacobian so it doesn’t learn crazy scalings.

Training Specialist: I suggest AdamW, weight decay about 1e-4, gradient clipping at 1.0. Cosine annealing with warm restarts and mixed precision should keep training stable. For robustness to dropout, we can inject small Gaussian noise into x_baseline and randomly mask 5% of features during training. Early stopping with patience around 20 should keep us from overfitting.

Single-Cell Biologist: We’ll need a way to encode cell-line specificity. A learned embedding per cell line that modulates both the baseline latent and the perturbation context should help. On evaluation, I’d like to see that, for example, in K562 under DNA damage agents, CDKN1A and TP53 pathway genes respond appropriately; in A549, EMT-related programs behave sensibly; and in MCF7, estrogen-related signatures are respected. Global MSE and PCC are fine, but we should include top-k DE-gene PCC per perturbation to capture salience.

Critic: Two flags. First, zero inflation: MSE in normalized space is pragmatic, but let’s at least compute metrics in centered, normalized space and not force the model to fit literal zeros. If time permits, we could add an auxiliary head with a count-aware loss as an ablation. Second, OT stability: if it turns brittle or too slow, we need a fallback (e.g., MMD-based alignment). Also, dose responses are not always monotonic; any monotonic constraint in the dose branch should be optional.

## Round 2

Data Scientist: I ran a quick pass: log2(CP10K+1) and gene-wise z-scoring largely neutralize plate-level scale shifts, but hash-related artifacts linger a bit. Rather than heavy-handed batch correction, I added a lightweight “batch token” that the encoder can learn to ignore. For features, a 4k HVG panel seems to balance signal and runtime. We’ll normalize log10-dose to [0, 1] and treat missing or constant doses carefully (zeros in those cases).

Model Architect: Let me make the mapping explicit. The baseline encoder (MLP with LayerNorm and GELU) maps the HVG-normalized x_baseline to z_control in 128–256 dimensions. The perturbation encoder has two parts: a learned embedding for the compound ID (say 128D) and a small MLP for the normalized log10-dose (2–3 layers with GELU), fused into a 128D context. We’ll multiply this context with a learned cell-line embedding to inject line-specific modulation. For the geometry, we compute an OT plan between the mini-batch of z_controls and the mini-batch of z_targets (targets are encoder outputs of observed perturbed cells), using a squared Euclidean cost. From the plan P, we produce an OT-guided latent z_ot = P @ z_target. Then a conditional flow refines z_control toward z_ot, conditioned on the perturbation context, giving z_refined. The decoder predicts Δx from z_refined. The final absolute prediction is x_pred = x_baseline + Δx, so inputs and outputs line up with the task.

Deep Learning Engineer: On the flow: six coupling layers, alternating masks, scale and translate nets are two-layer MLPs with GELU. We’ll add a small penalty on the log-determinant squared to discourage extreme volume changes. For OT, 40–60 Sinkhorn iterations, ε around 0.1 to start. If batches are too big, we’ll do blockwise transport or reduce the pair matrix via nearest-neighbor subsets; that’s a runtime engineering detail we can implement if needed.

Training Specialist: Losses need to respect this pipeline. Primary loss: MSE on Δx in the normalized HVG space, comparing to the observed delta (x_target − x_baseline). OT loss: average cost under P to anchor the geometry. Flow alignment: MSE between z_refined and z_ot, with z_ot detached to prevent gradient loops, plus 0.01 times mean(log_det^2). DE emphasis: add a weighted MSE on top-k DE genes per perturbation to highlight salient shifts. L2 regularization across weights. We’ll tune α, β, γ, and the DE weight via Optuna.

Single-Cell Biologist: For the DE term, we’ll define DE genes per perturbation by absolute fold change between perturbed and control means. As for evaluation, in addition to global metrics, we’ll run per-perturbation top-k PCC and perform pathway enrichment on predicted Δx. I’ll also prepare a marker-based checklist per cell line.

Critic: We’re assuming target latents z_target come from encoding the observed perturbed cells. Good—this anchors the OT to reality rather than hallucinations. But be careful with pairing: train-time batches should contain both controls and perturbed examples for the same and different compounds to prevent trivial matches. Also, consider a short warmup with no OT/flow so the encoder/decoder learn stable deltas before we add geometric constraints.

## Round 3

Data Scientist: I’ve finalized the training/test splits. We’ll have three regimes: (1) random splits; (2) held-out compounds (all doses); (3) held-out cell line (train on two lines, test on the third). All normalizers are fit on the training set only. During training, each batch includes a mix of lines, compounds, and doses, with both control and treated cells to make the OT meaningful. Data augmentation: small Gaussian noise on x_baseline and 5% random masking of features.

Model Architect: The decoder will benefit from modularity. We’ll partition the HVGs into ~10–12 groups, roughly by co-expression clusters or pathway membership. Each module head is a small MLP that outputs Δx for its subset; we also include a global residual path from z_refined to the full HVG space to ensure coverage. The perturbation context will gate the module outputs (think multiplicative attention), so kinase inhibitors can upweight kinase-related modules, HDAC inhibitors tune chromatin modules, etc.

Deep Learning Engineer: On dose: we’ll allow non-linear and even non-monotonic mapping by using GELU and optional higher-order interactions in the dose MLP. No hard monotonicity constraints unless we see clear evidence they help. I’ll also enable gradient checkpointing on the flow and decoder to fit larger effective batches with AMP. If OT becomes the bottleneck, we’ll fall back to fewer Sinkhorn steps and evaluate the trade-offs.

Training Specialist: Training schedule: Phase 1 (stabilization), 10–20 epochs optimizing only Δx reconstruction (and DE term) with the encoders and decoder. Phase 2, introduce OT and flow; unfreeze everything and continue 60–120 epochs with cosine restarts, warmup for the first five epochs, early stopping patience 20. We’ll monitor MSE, PCC, top-k PCC per perturbation, transport cost, and the flow’s Jacobian norms. Checkpoints saved on best validation loss.

Single-Cell Biologist: I’ll add biological sanity checks during training: for doxorubicin, look for CDKN1A upregulation in K562; for EGFR/MEK inhibitors, inspect MAPK pathway signals in A549; in MCF7, assess estrogen pathway activity under relevant drugs. We’ll also plot predicted vs. observed dose-response curves for sentinel genes per compound to catch dose encoding failures.

Critic: The modular decoder initialized by pathway clusters is attractive but could over-bias toward known biology. Keep the module assignments soft—let the model learn gates—and retain the global residual so novel patterns still emerge. Also ensure that when we report cross-line performance, we never leak line-specific normalization or embeddings through the splits.

## Round 4

Data Scientist: Preprocessing is locked in: log2(CP10K+1), per-gene z-scoring using training stats, genes detected in ≥1% of cells retained, HVGs around 4k (we’ll search 3–5k), compound ID as a learned embedding, normalized log10-dose in [0, 1]. During inference, we apply the exact same normalization maps and encode the requested compound+dose; the model outputs Δx over the HVGs, and we compute x_pred = x_baseline + Δx. Inputs and outputs are consistent end to end.

Model Architect: Final architecture: Baseline encoder (3-layer MLP with LayerNorm and GELU, dropout 0.1; latent 128–256). Perturbation encoder: 128D compound embedding plus a 2–3 layer dose MLP, fused and modulated by a learned cell-line embedding. Conditional OT: entropic OT on latent batches (ε≈0.1, ~50 iterations) to produce z_ot as a first-pass mapping; Flow refiner: six conditional masked affine coupling layers that gently nudge z_control toward z_ot. Decoder: ~12 module heads plus a global residual to emit Δx. The absolute target is reconstructed by adding Δx to the input baseline vector.

Training Specialist: Loss weights to start: Δx MSE α=1.0; OT cost β=0.1; flow alignment γ=0.1 with 0.01× mean(log_det^2); DE emphasis η=0.5; L2 regularization 1e-5. Optimizer: AdamW (lr 1e-3 → cosine restarts, eta_min 1e-6), gradient clipping 1.0, mixed precision, gradient accumulation ×4. We’ll run Optuna to fine-tune α, β, γ, η and a few architecture choices like latent size and number of module heads.

Deep Learning Engineer: Practicalities: gradient checkpointing on the flow and decoder, balanced batch composition across compounds/lines/doses, deterministic pairing at test time for reproducible metrics. If OT is unstable, we’ll lower the iteration count, increase ε slightly, or switch to an MMD alignment ablation and document the impact. If flows overfit, we’ll cut to four layers and raise the Jacobian penalty; worst case, replace with a residual MLP refiner.

Single-Cell Biologist: Evaluation will include global MSE and PCC, top-k DE PCC per perturbation, pathway enrichment on predicted Δx, and visual dose-response calibration curves for key drug–gene pairs. For generalization, we’ll emphasize held-out compounds and the held-out line setting. Interpretability comes from the gating on module heads and from perturbation/line embeddings we can probe post hoc.

Critic: Final checks: (1) The objective matches the task: input baseline + perturbation descriptor, output Δx, reconstruct x_pred = baseline + Δx. (2) The geometry constraint (OT) is advisory rather than dictatorial; we detach z_ot in the flow alignment to avoid gradient loops. (3) The training schedule avoids early instability by deferring OT/flow until the base mapping is learned. (4) We have explicit fallbacks if OT or flow become liabilities. With these in place, the derivation from dataset features and baseline shortcomings to this architecture is coherent: delta learning for cross-line generalization, OT to preserve latent geometry under treatment, conditional flows to capture non-linear, dose- and drug-specific effects, and a modular decoder to respect multi-gene covariance.

Conclusion (consensus): We proceed with the conditional OT plus flow-refined delta predictor, trained in two phases, evaluated with both global and DE-focused metrics, and validated biologically within each cell line. This design traces cleanly from the drug6 data characteristics to the final input/output behavior and provides clear knobs for stability, scalability, and interpretability.